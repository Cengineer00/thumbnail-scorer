{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN AND SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "**Pre-trained Vision Model + Regressor**\n",
    "\n",
    "**Pre-trained Vision Model:**\n",
    "\n",
    "- CLIP\n",
    "- ResNet50\n",
    "- â€¦\n",
    "\n",
    "**Regressor:**\n",
    "\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- MLP\n",
    "- â€¦\n",
    "\n",
    "Vision model will produce image embeddings. Regressor will take it and give a score.\n",
    "\n",
    "We will only train regressor head of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Why CLIP ResNet-50 + LightGBM?\n",
    "\n",
    "### ðŸ” CLIP ResNet-50 (RN50)\n",
    "- **Pretrained** on massive datasets â†’ captures general visual features well\n",
    "- **ResNet backbone** is lighter and faster than ViT â†’ ideal for CPU/MacBook M3\n",
    "- No fine-tuning needed â†’ use as a frozen image encoder\n",
    "- Extracts 1024D embeddings â†’ perfect for downstream ML models\n",
    "\n",
    "### âš¡ LightGBM Regressor\n",
    "- Extremely **fast and efficient on CPU**\n",
    "- Handles **small to medium datasets** well\n",
    "- Works great with **numerical features** like image embeddings\n",
    "- Easy to **train, save, and interpret**\n",
    "\n",
    "### ðŸ§© Architecture Summary\n",
    "YouTube Thumbnail â†’ CLIP-RN50 â†’ 1024D Embedding â†’ LightGBM â†’ Performance Score\n",
    "\n",
    "\n",
    "âœ… Lightweight, fast, and effective pipeline for thumbnail scoring on local machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have thumbnails and their metadata stored. We should create their embeddings from the images and label them with a score function to feed the regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CLIP ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orhun/miniconda3/envs/TScorer/lib/python3.11/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): ModifiedResNet(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU(inplace=True)\n",
       "    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (attnpool): AttentionPool2d(\n",
       "      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (c_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('RN50', pretrained='openai')\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need a scoring function to label thumbnails using their metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "def score_thumbnails(metadata: DataFrame) -> DataFrame:\n",
    "\n",
    "    metadata['score'] = np.log(metadata['view_count'] / metadata['average_view_count'])\n",
    "    \n",
    "    return metadata.sort_values(by='score', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "df = pd.read_csv(\"data/raw/total.csv\")\n",
    "\n",
    "# Filter and score thumbnails\n",
    "df = score_thumbnails(df)\n",
    "\n",
    "# Save the filtered and scored thumbnails\n",
    "df.to_csv(\"data/raw/scored_metadata.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Rescale Scores Based on Percentiles\n",
    "To normalize the score column into a new range between 0.5 and 10 based on percentiles, we will log-based scores into a more intuitive and comparable scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated scores saved to data/raw/scored_metadata_scaled.csv\n"
     ]
    }
   ],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(\"data/raw/scored_metadata.csv\")\n",
    "\n",
    "# Define number of bins (e.g., 20 bins between 0.5 and 10)\n",
    "n_bins = 20\n",
    "bin_edges = np.percentile(df['score'], np.linspace(0, 100, n_bins + 1))\n",
    "bin_labels = np.round(np.linspace(0.5, 10, n_bins), 2)\n",
    "\n",
    "# Digitize score into bins\n",
    "df['scaled_score'] = pd.cut(df['score'], bins=bin_edges, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "# Optional: convert to float\n",
    "df['scaled_score'] = df['scaled_score'].astype(float)\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(\"data/raw/scored_metadata_scaled.csv\", index=False)\n",
    "\n",
    "print(\"Updated scores saved to data/raw/scored_metadata_scaled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CLIP ResNet50 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Qx18v00TVqI: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/Qx18v00TVqI.jpg'\n",
      "Skipping pk9924dUdz4: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/pk9924dUdz4.jpg'\n",
      "Skipping RP7dF9vXqZY: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/RP7dF9vXqZY.jpg'\n",
      "Skipping 7tTzSeWGY90: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/7tTzSeWGY90.jpg'\n",
      "Skipping behMZUjf_Rw: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/behMZUjf_Rw.jpg'\n",
      "Skipping K_ovEr89zyg: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/K_ovEr89zyg.jpg'\n",
      "Skipping VnupfBL9DJ8: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/VnupfBL9DJ8.jpg'\n",
      "Skipping sn7_X2zjfLA: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/sn7_X2zjfLA.jpg'\n",
      "Skipping wPTOq3opOys: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/wPTOq3opOys.jpg'\n",
      "Skipping _tPb-ZrMxw0: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/_tPb-ZrMxw0.jpg'\n",
      "Skipping zvTzbBp9GpY: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/zvTzbBp9GpY.jpg'\n",
      "Skipping aQREK1342sA: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/aQREK1342sA.jpg'\n",
      "Skipping 6wTaDhQUNzU: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/6wTaDhQUNzU.jpg'\n",
      "Skipping p5zqk_9YwUI: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/p5zqk_9YwUI.jpg'\n",
      "Skipping on8DpRzAezQ: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/on8DpRzAezQ.jpg'\n",
      "Skipping SixhwVaKTH4: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/SixhwVaKTH4.jpg'\n",
      "Skipping HzSOIIw6CuQ: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/HzSOIIw6CuQ.jpg'\n",
      "Skipping YroFov0nqQ8: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/YroFov0nqQ8.jpg'\n",
      "Skipping xfDGwSr151M: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/xfDGwSr151M.jpg'\n",
      "Skipping 6iseB2mrNGE: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/6iseB2mrNGE.jpg'\n",
      "Skipping m5PGMaxX9CA: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/m5PGMaxX9CA.jpg'\n",
      "Skipping tGh-P7aGQE4: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/tGh-P7aGQE4.jpg'\n",
      "Skipping qYm17iTfz9U: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/qYm17iTfz9U.jpg'\n",
      "Skipping GweSFdvkoe8: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/GweSFdvkoe8.jpg'\n",
      "Skipping V09qHAW-4YY: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/V09qHAW-4YY.jpg'\n",
      "Skipping RFowi-HklzA: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/RFowi-HklzA.jpg'\n",
      "Skipping Q_WyaiXK4l0: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/Q_WyaiXK4l0.jpg'\n",
      "Skipping pH_udFFYWR4: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/pH_udFFYWR4.jpg'\n",
      "Skipping R7j679WFS10: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/R7j679WFS10.jpg'\n",
      "Skipping wY6txfDdwog: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/wY6txfDdwog.jpg'\n",
      "Skipping QLH8rlwn-xc: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/QLH8rlwn-xc.jpg'\n",
      "Skipping s_W1S2iZ9qE: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/s_W1S2iZ9qE.jpg'\n",
      "Skipping no0912_652Y: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/no0912_652Y.jpg'\n",
      "Skipping MdQBMbq3Kl4: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/MdQBMbq3Kl4.jpg'\n",
      "Skipping zE6s-hsV1bI: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/zE6s-hsV1bI.jpg'\n",
      "Skipping CnstxbgXH9w: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/CnstxbgXH9w.jpg'\n",
      "Skipping q4l3WHmGGJQ: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/q4l3WHmGGJQ.jpg'\n",
      "Skipping ji1msj3uuns: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/ji1msj3uuns.jpg'\n",
      "Skipping pawQA3UB53M: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/pawQA3UB53M.jpg'\n",
      "Skipping Tpni_JjEay0: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/Tpni_JjEay0.jpg'\n",
      "Skipping dGXjBG8PYIA: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/dGXjBG8PYIA.jpg'\n",
      "Skipping MaMyg9MAJ_o: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/MaMyg9MAJ_o.jpg'\n",
      "Skipping pvBHyip4peo: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/pvBHyip4peo.jpg'\n",
      "Skipping 4M2iCz0nEQ4: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/4M2iCz0nEQ4.jpg'\n",
      "Skipping 4p4m_uTOFaI: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/4p4m_uTOFaI.jpg'\n",
      "Skipping LJmKZVrv2hQ: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/LJmKZVrv2hQ.jpg'\n",
      "Skipping gvPnhxS4i4I: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/gvPnhxS4i4I.jpg'\n",
      "Skipping I2r9QkdDGSQ: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/I2r9QkdDGSQ.jpg'\n",
      "Skipping S_YUp8-X_ZA: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/S_YUp8-X_ZA.jpg'\n",
      "Skipping w_oFc5evPKQ: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/w_oFc5evPKQ.jpg'\n",
      "Skipping fLV0y0Dn0Ug: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/fLV0y0Dn0Ug.jpg'\n",
      "Skipping 7gEbHsHXdn0: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/7gEbHsHXdn0.jpg'\n",
      "Skipping jOe8OBv0rIU: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/jOe8OBv0rIU.jpg'\n",
      "Skipping rroSPpHhEvs: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/rroSPpHhEvs.jpg'\n",
      "Skipping -krl_BUineQ: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/-krl_BUineQ.jpg'\n",
      "Skipping XAZIZyny1rE: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/XAZIZyny1rE.jpg'\n",
      "Skipping bMpUIJqKZHA: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/bMpUIJqKZHA.jpg'\n",
      "Skipping wYCfIyY6VGQ: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/wYCfIyY6VGQ.jpg'\n",
      "Skipping hDpU3UMj_ss: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/hDpU3UMj_ss.jpg'\n",
      "Skipping 7ap3CkBEpOE: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/7ap3CkBEpOE.jpg'\n",
      "Skipping 7CS5G_tzeM4: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/7CS5G_tzeM4.jpg'\n",
      "Skipping tEUf0kMemM8: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/tEUf0kMemM8.jpg'\n",
      "Skipping 8ryS35XtOvQ: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/8ryS35XtOvQ.jpg'\n",
      "Skipping uV8ELH0C0wc: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/uV8ELH0C0wc.jpg'\n",
      "Skipping X0WR3ZbeFqA: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/X0WR3ZbeFqA.jpg'\n",
      "Skipping a4c4cO3ufaU: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/a4c4cO3ufaU.jpg'\n",
      "Skipping o07OUE-bgPo: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/o07OUE-bgPo.jpg'\n",
      "Skipping D5ExcrkTBrw: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/D5ExcrkTBrw.jpg'\n",
      "Skipping w6OTch1ZsCw: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/w6OTch1ZsCw.jpg'\n",
      "Skipping 0hHAc3nSrEk: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/0hHAc3nSrEk.jpg'\n",
      "Skipping 5zeEXOSVBO0: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/5zeEXOSVBO0.jpg'\n",
      "Skipping TPcFEWbgjVo: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/TPcFEWbgjVo.jpg'\n",
      "Skipping ZyU6sxEfk3c: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/ZyU6sxEfk3c.jpg'\n",
      "Skipping 3nncy0zRVXw: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/3nncy0zRVXw.jpg'\n",
      "Skipping G8HEWefnOmY: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/G8HEWefnOmY.jpg'\n",
      "Skipping JZjHSF6xeDc: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/JZjHSF6xeDc.jpg'\n",
      "Skipping ZiRd-hA1wew: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/ZiRd-hA1wew.jpg'\n",
      "Skipping b1FkrFfStLc: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/b1FkrFfStLc.jpg'\n",
      "Skipping Fko84aVnR28: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/Fko84aVnR28.jpg'\n",
      "Skipping UNzSesPTE7U: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/UNzSesPTE7U.jpg'\n",
      "Skipping rn3OlzWqnU4: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/rn3OlzWqnU4.jpg'\n",
      "Skipping QSA744daJo4: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/QSA744daJo4.jpg'\n",
      "Skipping rsVgMjXfvuo: [Errno 2] No such file or directory: 'src/data_scraping/thumbnails/rsVgMjXfvuo.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 94/94 [00:25<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Batch processing complete\n",
      "X_train shape: (2996, 1024)\n",
      "y_train shape: (2996,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the folder containing thumbnail images\n",
    "image_folder = \"src/data_scraping/thumbnails\"\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "image_tensors = []\n",
    "scores = []\n",
    "X_train = []\n",
    "\n",
    "# Prepare data for batching\n",
    "valid_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    image_path = os.path.join(image_folder, row['video_id'])+\".jpg\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = preprocess(image)\n",
    "        image_tensors.append(image_tensor)\n",
    "        scores.append(row['scaled_score'])\n",
    "        valid_rows.append(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {row['video_id']}: {e}\")\n",
    "\n",
    "# Batch processing\n",
    "for i in tqdm(range(0, len(image_tensors), batch_size)):\n",
    "    batch = image_tensors[i:i+batch_size]\n",
    "    batch_tensor = torch.stack(batch).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model.encode_image(batch_tensor).cpu().numpy()\n",
    "\n",
    "    X_train.extend(batch_embeddings)\n",
    "\n",
    "# Convert to final arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(scores[:len(X_train)])\n",
    "\n",
    "print(\"âœ… Batch processing complete\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 261120\n",
      "[LightGBM] [Info] Number of data points in the train set: 2996, number of used features: 1024\n",
      "[LightGBM] [Info] Start training from score 5.252003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model.lgb']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "# Save the trained model\n",
    "joblib.dump(lgb_model, \"model.lgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Thumbnail Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "regressor: lgb.LGBMRegressor\n",
    "regressor = joblib.load(\"model.lgb\")\n",
    "\n",
    "# --- Feature extraction ---\n",
    "def extract_image_embedding(image_path):\n",
    "    image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "    return image_features.cpu().numpy()\n",
    "\n",
    "# --- Inference function ---\n",
    "def predict_thumbnail_score(image_path):\n",
    "    embedding = extract_image_embedding(image_path)\n",
    "    score = regressor.predict(embedding)[0]\n",
    "    est_ratio = np.exp(score)\n",
    "    return {\n",
    "        \"log_score\": round(score, 4),\n",
    "        \"views_per_subscriber_est\": round(est_ratio, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: {'log_score': np.float64(-0.9701), 'views_per_subscriber_est': np.float64(0.38)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orhun/miniconda3/envs/TScorer/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_path = \"noktali_virgul_podcast_thumbnails/kq38Urs5QEU.jpg\"  # Replace with your own image\n",
    "result = predict_thumbnail_score(image_path)\n",
    "print(\"Predicted:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: {'log_score': np.float64(4.9671), 'views_per_subscriber_est': np.float64(143.61)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orhun/miniconda3/envs/TScorer/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_path = \"noktali_virgul_podcast_thumbnails/kq38Urs5QEU.jpg\"  # Replace with your own image\n",
    "result = predict_thumbnail_score(image_path)\n",
    "print(\"Predicted:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TScorer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
